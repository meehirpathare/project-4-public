{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='blue'>This Jupyter Notebook fits corpus to kmeans, plots elbow graphs, plots TSNE, plots kmeans, and prints word clusters</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Import necessary packages and define functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import collections\n",
    "from cleaner_funcs import clean, clean_text_string, clean_list\n",
    "import csv\n",
    "from helpers import top_features, add_stopwords, get_kmeans_wcss, get_top_keywords, plot_tsne_pca, load_csv\n",
    "import json\n",
    "import matplotlib.cm as cm\n",
    "from matplotlib import pyplot as plt\n",
    "from NLPPipe import NLPPipe\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize, RegexpTokenizer, TreebankWordTokenizer\n",
    "from nltk.util import ngrams\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_extraction import text\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.manifold import TSNE\n",
    "import seaborn as sns\n",
    "from textblob import TextBlob\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Print Current Working Directory "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\Meehir\\\\Documents\\\\GitHub\\\\project-4-public\\\\notebooks'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Read in JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = 'C:\\\\Users\\\\Meehir\\\\Documents\\\\GitHub\\\\project-4-public\\\\data\\\\'\n",
    "\n",
    "with open(data_path + 'json_files\\\\combined_2000.json', 'r', encoding='utf-8') as f:\n",
    "    data = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Build corpus from comment text strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "comms = []\n",
    "\n",
    "for thread in data:\n",
    "    for comment in thread['comments']:\n",
    "        comms.append(comment['comment'])\n",
    "        \n",
    "corpus = comms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 Loading in stop words. See [preprocess](preprocess.ipynb) notebook for further details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_word_loc = data_path + \"\\\\csv_files\\\\\"\n",
    "my_stop_words = load_csv(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_to_del = ['ve', 'thi', 'wa', 'like', 'year','just', 'know', 'good', 'don', 'ani', 'want', 'realli', 'veri',\n",
    "                'ha', 'thank','say', 'got', 'becaus', 'look', 'make', 'time', 'ha', 'did', 'peopl', 'doe', 'sure',\n",
    "                'need', 'whi', 'lot', 'thing', 'state', 'gener', 'great', 'anyon', 'probabl', 'hi', 'tri', 'sinc', \n",
    "                'ec','way', 'said', 'll', 'write', 'high', 'place', 'didn', 'everyon', 'pretti', 'come', 'ye', 'ca', 'ask', 'els', 'use', 'ii', 'doesn',\n",
    "                'ivi', 'person', 'http', 'kid', 'day', 'talk', 'number', 'start', 'anoth', 'anoth', 'end', 'rd', \n",
    "                'someth', 'mayb', 'anyth', 'possibl', 'thread', 'bc', 'wonder', 'alreadi','took', 'hour', 'www', \n",
    "                'everi', 'abl', 'went', 'let', 'load', 'heard', 'tell', 'live', 'sea', 'big', 'pleas',\n",
    "                'someon', 'chang', 'inform', 'bit', 'urm', 'guess', 'mention', 'taken', 'dure', 'believ', \n",
    "                'com', 'especi','isn', 'abov', 'lol', 'compar', 'wrote', 'agre', 'object', 'non', 'noth', \n",
    "                'alway', 'edu', 'hey', 'sent','ab', 'kind', 'given', 'overal', 'happen', 'singl', 'pre', 'penn', 'upenn']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_stop_words = add_stopwords(my_stop_words, words_to_del)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aa</th>\n",
       "      <th>aaa</th>\n",
       "      <th>aaaa</th>\n",
       "      <th>aaaaa</th>\n",
       "      <th>aaaaahhhhh</th>\n",
       "      <th>aaaand</th>\n",
       "      <th>aaand</th>\n",
       "      <th>aaja</th>\n",
       "      <th>aapplic</th>\n",
       "      <th>aapt</th>\n",
       "      <th>...</th>\n",
       "      <th>zonumh</th>\n",
       "      <th>zoo</th>\n",
       "      <th>zoolog</th>\n",
       "      <th>zoom</th>\n",
       "      <th>zuckerberg</th>\n",
       "      <th>zuckerman</th>\n",
       "      <th>zurich</th>\n",
       "      <th>zxcvbnmwait</th>\n",
       "      <th>zzeemmnntt</th>\n",
       "      <th>Ã©tat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16446</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16447</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16448</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16449</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16450</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>16451 rows Ã 13978 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        aa  aaa  aaaa  aaaaa  aaaaahhhhh  aaaand  aaand  aaja  aapplic  aapt  \\\n",
       "0      0.0  0.0   0.0    0.0         0.0     0.0    0.0   0.0      0.0   0.0   \n",
       "1      0.0  0.0   0.0    0.0         0.0     0.0    0.0   0.0      0.0   0.0   \n",
       "2      0.0  0.0   0.0    0.0         0.0     0.0    0.0   0.0      0.0   0.0   \n",
       "3      0.0  0.0   0.0    0.0         0.0     0.0    0.0   0.0      0.0   0.0   \n",
       "4      0.0  0.0   0.0    0.0         0.0     0.0    0.0   0.0      0.0   0.0   \n",
       "...    ...  ...   ...    ...         ...     ...    ...   ...      ...   ...   \n",
       "16446  0.0  0.0   0.0    0.0         0.0     0.0    0.0   0.0      0.0   0.0   \n",
       "16447  0.0  0.0   0.0    0.0         0.0     0.0    0.0   0.0      0.0   0.0   \n",
       "16448  0.0  0.0   0.0    0.0         0.0     0.0    0.0   0.0      0.0   0.0   \n",
       "16449  0.0  0.0   0.0    0.0         0.0     0.0    0.0   0.0      0.0   0.0   \n",
       "16450  0.0  0.0   0.0    0.0         0.0     0.0    0.0   0.0      0.0   0.0   \n",
       "\n",
       "       ...  zonumh  zoo  zoolog  zoom  zuckerberg  zuckerman  zurich  \\\n",
       "0      ...     0.0  0.0     0.0   0.0         0.0        0.0     0.0   \n",
       "1      ...     0.0  0.0     0.0   0.0         0.0        0.0     0.0   \n",
       "2      ...     0.0  0.0     0.0   0.0         0.0        0.0     0.0   \n",
       "3      ...     0.0  0.0     0.0   0.0         0.0        0.0     0.0   \n",
       "4      ...     0.0  0.0     0.0   0.0         0.0        0.0     0.0   \n",
       "...    ...     ...  ...     ...   ...         ...        ...     ...   \n",
       "16446  ...     0.0  0.0     0.0   0.0         0.0        0.0     0.0   \n",
       "16447  ...     0.0  0.0     0.0   0.0         0.0        0.0     0.0   \n",
       "16448  ...     0.0  0.0     0.0   0.0         0.0        0.0     0.0   \n",
       "16449  ...     0.0  0.0     0.0   0.0         0.0        0.0     0.0   \n",
       "16450  ...     0.0  0.0     0.0   0.0         0.0        0.0     0.0   \n",
       "\n",
       "       zxcvbnmwait  zzeemmnntt  Ã©tat  \n",
       "0              0.0         0.0   0.0  \n",
       "1              0.0         0.0   0.0  \n",
       "2              0.0         0.0   0.0  \n",
       "3              0.0         0.0   0.0  \n",
       "4              0.0         0.0   0.0  \n",
       "...            ...         ...   ...  \n",
       "16446          0.0         0.0   0.0  \n",
       "16447          0.0         0.0   0.0  \n",
       "16448          0.0         0.0   0.0  \n",
       "16449          0.0         0.0   0.0  \n",
       "16450          0.0         0.0   0.0  \n",
       "\n",
       "[16451 rows x 13978 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp = NLPPipe(vectorizer=TfidfVectorizer(stop_words=set(my_stop_words),max_features=25000), \n",
    "              cleaning_function=clean, \n",
    "              tokenizer=TreebankWordTokenizer().tokenize, \n",
    "              stemmer=PorterStemmer())\n",
    "\n",
    "nlp.fit(corpus)\n",
    "X = nlp.transform(corpus).toarray()\n",
    "df = pd.DataFrame(X, columns = nlp.vectorizer.get_feature_names())\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(nlp.vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16451, 13978)\n"
     ]
    }
   ],
   "source": [
    "nlp.fit(corpus)\n",
    "vectors = nlp.transform(corpus)\n",
    "\n",
    "X_data = vectors.todense()\n",
    "print(X_data.shape)\n",
    "\n",
    "wcss=[]\n",
    "\n",
    "for i in range(1,25): \n",
    "    kmeans = KMeans(n_clusters=i, init ='k-means++', max_iter=300,  n_init=10,random_state=0)\n",
    "    kmeans.fit(X_data)\n",
    "\n",
    "    wcss.append(kmeans.inertia_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(wcss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,10))\n",
    "plt.plot(range(1,11),wcss)\n",
    "plt.title('The Elbow Method Graph')\n",
    "plt.xlabel('Number of clusters')\n",
    "plt.ylabel('WCSS')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp2, wcss2 = get_kmeans_wcss(my_stop_words, 250, corpus)\n",
    "nlp3, wcss3 = get_kmeans_wcss(my_stop_words, 2500, corpus)\n",
    "nlp4, wcss4 = get_kmeans_wcss(my_stop_words, 1000, corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,10))\n",
    "plt.plot(range(1,11),wcss2, '-*', label = 'top 250 words', linewidth = 4, markersize=25)\n",
    "plt.plot(range(1, 11), wcss, '-p', label = 'all 13899 words', linewidth = 4,markersize=25)\n",
    "plt.plot(range(1,11), wcss3, '-h', label = 'top 2500 words', linewidth = 4,markersize=25)\n",
    "plt.plot(range(1,11), wcss4, '-o', label = 'top 1000 words', linewidth = 4,markersize=25)\n",
    "plt.title('The Elbow Graph', fontsize=32)\n",
    "plt.xlabel('Number of clusters', fontsize=20)\n",
    "plt.ylabel('WCSS', fontsize=20)\n",
    "plt.xticks(fontsize=14)\n",
    "plt.yticks(fontsize=14)\n",
    "plt.legend(fontsize='xx-large')\n",
    "plt.savefig('elbow.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['1', '2', '3', '4', '5', '6', '7', '8', '9', '10']\n",
    "ind = ['All 13899 words', 'Top 2500 words', 'Top 1000 words', 'Top 250 words']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame([wcss, wcss3, wcss4, wcss2], columns = cols)\n",
    "df = df.set_axis(ind, inplace=False).set_axis(cols, axis=1, inplace=False)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=7, init ='k-means++', max_iter=300, n_init=10,random_state=0)\n",
    "clusters = kmeans.fit_predict(X_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_keywords(df, clusters, labels, n_terms):\n",
    "    \n",
    "    for i,r in df.iterrows():\n",
    "        print('\\nCluster {}'.format(i))\n",
    "        print(','.join([labels[t] for t in np.argsort(r)[-n_terms:]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_top_keywords(df, clusters, nlp.vectorizer.get_feature_names(), 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_tsne_pca(X_data, clusters)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
