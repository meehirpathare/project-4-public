{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='blue'>This Jupyter Notebook processes text, builds corpus, and explores the most common features </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import necessary packages and helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cleaner_funcs import clean, clean_text_string, clean_list\n",
    "import collections\n",
    "import csv\n",
    "from helpers import add_stopwords, load_csv, save_to_csv\n",
    "from NLP_functions import add_stopwords, get_top_keywords, top_features\n",
    "import json\n",
    "from NLPPipe import NLPPipe\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize, RegexpTokenizer, TreebankWordTokenizer\n",
    "from nltk.util import ngrams\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from sklearn.feature_extraction import text\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from textblob import TextBlob\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting path to data files and checking current working directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\Meehir\\\\Documents\\\\GitHub\\\\project-4-public\\\\notebooks'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_path = 'C:\\\\Users\\\\Meehir\\\\Documents\\\\GitHub\\\\project-4-public\\\\data\\\\'\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading in comment data from JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(data_path + 'json_files\\\\combined_2000.json', 'r', encoding='utf-8') as f:\n",
    "    data = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making a list of comment content strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "comms = []\n",
    "\n",
    "for thread in data:\n",
    "    for comment in thread['comments']:\n",
    "        comms.append(comment['comment'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking length (i.e. number of comments scraped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16451"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(comms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Copying to variable <font color='blue'>*corpus*</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = comms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load stop words from package "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_stop_words = list(text.ENGLISH_STOP_WORDS.union([\"book\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make nlp object\n",
    "    1. fit model to corpus\n",
    "    2. transform model \n",
    "    3. convert to dense array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp = NLPPipe(vectorizer=TfidfVectorizer(stop_words=set(my_stop_words),max_features=15000), \n",
    "              cleaning_function=clean, \n",
    "              tokenizer=TreebankWordTokenizer().tokenize, \n",
    "              stemmer=PorterStemmer())\n",
    "\n",
    "nlp.fit(corpus)\n",
    "nlp.transform(corpus).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['penn', 'school', 'thi', 'appli', 'year', 'student', 'wa', 'like', 'think', 'just', 'applic', 'know', 'colleg', 'good', 'don', 'accept', 'ani', 'wharton', 'class', 'want', 'realli', 'veri', 'ha', 'ed', 'upenn', 'onli', 'major', 'gpa', 'thank', 'program', 'score', 'sat', 'admiss', 'chanc', 'work', 'time', 'make', 'say', 'got', 'becaus', 'look', 'did', 'peopl', 'help', 'essay', 'decis', 'high', 'mani', 'math', 'doe', 'sure', 'need', 'cours', 'ap', 'whi', 'best', 'lot', 'act', 'thing', 'test', 'hope', 'experi', 'state', 'reject', 'univers', 'busi', 'differ', 'gener', 'great', 'scienc', 'anyon', 'subject', 'competit', 'financi', 'probabl', 'commun', 'recommend', 'hi', 'tri', 'aid', 'grade', 'club', 'stat', 'interview', 'rank', 'senior', 'sinc', 'intern', 'ec', 'place', 've', 'way', 'transfer', 'rate', 'said', 'luck', 'summer', 'll', 'write', 'app']\n"
     ]
    }
   ],
   "source": [
    "print(top_features(nlp, 0, 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding to stop words from top 100 terms..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_to_del = ['ve', 'thi', 'wa', 'like', 'year','just', 'know', 'good', 'don', 'ani', 'want', 'realli', 'veri',\n",
    "                     'ha', 'thank','say', 'got', 'becaus', 'look', 'make', 'time', 'ha', 'did', 'peopl', 'doe', 'sure',\n",
    "                     'need', 'whi', 'lot', 'thing', 'state', 'gener', 'great', 'anyon', 'probabl', 'hi', 'tri', 'sinc', 'ec',\n",
    "                     'way', 'said', 'll', 'write']\n",
    "\n",
    "my_stop_words = add_stopwords(my_stop_words, words_to_del)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rerunning nlp model to account for new stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp = NLPPipe(vectorizer=TfidfVectorizer(stop_words=set(my_stop_words),max_features=15000), \n",
    "              cleaning_function=clean, \n",
    "              tokenizer=TreebankWordTokenizer().tokenize, \n",
    "              stemmer=PorterStemmer())\n",
    "\n",
    "nlp.fit(corpus)\n",
    "nlp.transform(corpus).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['penn', 'school', 'appli', 'student', 'think', 'applic', 'colleg', 'accept', 'wharton', 'class', 'ed', 'upenn', 'onli', 'major', 'gpa', 'program', 'score', 'sat', 'admiss', 'chanc', 'work', 'help', 'essay', 'decis', 'high', 'mani', 'math', 'cours', 'ap', 'best', 'act', 'test', 'hope', 'experi', 'reject', 'univers', 'busi', 'differ', 'scienc', 'subject', 'competit', 'financi', 'commun', 'recommend', 'aid', 'grade', 'club', 'stat', 'interview', 'rank', 'senior', 'intern', 'place', 'transfer', 'rate', 'summer', 'luck', 'app', 'better', 'didn', 'everyon', 'pretti', 'come', 'question', 'read', 'ye', 'award', 'admit', 'ca', 'mean', 'research', 'ask', 'definit', 'extracurricular', 'offer', 'consid', 'incom', 'els', 'use', 'ii', 'doesn', 'ivi', 'attend', 'person', 'http', 'academ', 'submit', 'feel', 'volunt', 'post', 'job', 'kid', 'studi', 'day', 'receiv', 'talk', 'engin', 'number', 'email', 'start']\n"
     ]
    }
   ],
   "source": [
    "print(top_features(nlp, 0, 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = NLPPipe(vectorizer=TfidfVectorizer(stop_words=set(my_stop_words),max_features=15000), \n",
    "              cleaning_function=clean, \n",
    "              tokenizer=TreebankWordTokenizer().tokenize, \n",
    "              stemmer=PorterStemmer())\n",
    "\n",
    "nlp.fit(corpus)\n",
    "nlp.transform(corpus).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(top_features(nlp, 100,250))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_to_del = ['high', 'place', 'didn', 'everyon', 'pretti', 'come', 'ye', 'ca', 'ask', 'els', 'use', 'ii', 'doesn',\n",
    "                'ivi', 'person', 'http', 'kid', 'day', 'talk', 'number', 'start']\n",
    "\n",
    "my_stop_words = add_stopwords(my_stop_words, words_to_del)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_to_del = ['didn', 'everyon', 'come', 'read', 'ye', 'els','anoth', 'anoth', 'end', 'rd', 'someth', 'mayb', 'anyth', 'possibl', 'thread', 'bc', 'wonder', 'alreadi',\n",
    "                'took', 'use', 'doesn', 'ii', 'ivi', 'http', 'howev', 'right', 'send', 'addit', 'befor', 'choic', 'anoth', 'end', 'someth', 'earli', 'bc', 'alreadi', \n",
    "                'hour', 'www', 'everi', 'abl', 'went', 'let', 'load', 'heard', 'tell', 'live', 'sea', 'big', 'pleas', 'anyth', 'guy', 'took', 'www', 'everi', 'went', 'abl', 'weight',\n",
    "                'includ', 'big', 'someon', 'bit', 'urm','someon', 'chang', 'inform', 'bit', 'urm', 'guess', 'mention', 'taken', 'dure', 'believ', 'com', 'especi',\n",
    "                'isn', 'abov', 'lol', 'compar', 'wrote', 'agre', 'object', 'non', 'noth', 'alway', 'edu', 'hey', 'sent']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_stop_words = add_stopwords(my_stop_words, words_to_del)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(top_features(nlp, 100, 250))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_to_del = ['ab', 'kind', 'given', 'overal', 'happen', 'singl', 'pre']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_stop_words = add_stopwords(my_stop_words, words_to_del)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(my_stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_to_csv(my_stop_words, data_path + \"\\\\csv_files\\\\\",'my_stop_words.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
