{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize, sent_tokenize, RegexpTokenizer\n",
    "from nltk.util import ngrams\n",
    "\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pickle\n",
    "import json\n",
    "from NLPPipe import NLPPipe\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from cleaner_funcs import clean\n",
    "from NLPPipe import NLPPipe\n",
    "\n",
    "import pandas as pd\n",
    "import collections\n",
    "\n",
    "from textblob import TextBlob\n",
    "from cleaner_funcs import clean, clean_text_string\n",
    "from sklearn.feature_extraction import text\n",
    "from helpers import top_features, add_stopwords\n",
    "\n",
    "import csv\n",
    "from cleaner_funcs import clean_list\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.chunk import ne_chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('combined_2000.json', 'r', encoding='utf-8') as f:\n",
    "    data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "comms = []\n",
    "\n",
    "for thread in data:\n",
    "    for comment in thread['comments']:\n",
    "        comms.append(comment['comment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16451"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(comms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = comms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_stop_words = list(text.ENGLISH_STOP_WORDS.union([\"book\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp = NLPPipe(vectorizer=TfidfVectorizer(stop_words=set(my_stop_words),max_features=15000), \n",
    "              cleaning_function=clean, \n",
    "              tokenizer=TreebankWordTokenizer().tokenize, \n",
    "              stemmer=PorterStemmer())\n",
    "\n",
    "nlp.fit(corpus)\n",
    "nlp.transform(corpus).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_100 = top_features(nlp, 0, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['penn', 'school', 'thi', 'appli', 'year', 'student', 'wa', 'like', 'just', 'think', 'applic', 'know', 'colleg', 'good', 'don', 'accept', 'ani', 'wharton', 'class', 'want', 'realli', 'veri', 'ha', 'ed', 'upenn', 'onli', 'major', 'gpa', 'program', 'thank', 'score', 'sat', 'admiss', 'chanc', 'work', 'time', 'make', 'say', 'got', 'becaus', 'look', 'did', 'peopl', 'help', 'essay', 'decis', 'high', 'mani', 'math', 'sure', 'doe', 'need', 'cours', 'ap', 'whi', 'best', 'lot', 'act', 'thing', 'test', 'hope', 'experi', 'state', 'reject', 'univers', 'busi', 'differ', 'gener', 'anyon', 'scienc', 'great', 'subject', 've', 'competit', 'financi', 'probabl', 'commun', 'recommend', 'hi', 'tri', 'aid', 'grade', 'club', 'stat', 'interview', 'rank', 'senior', 'sinc', 'intern', 'ec', 'place', 'way', 'transfer', 'said', 'rate', 'luck', 'summer', 'll', 'write', 'app']\n"
     ]
    }
   ],
   "source": [
    "print(top_100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_to_del = ['ve', 'thi', 'wa', 'like', 'year','just', 'know', 'good', 'don', 'ani', 'want', 'realli', 'veri',\n",
    "                     'ha', 'thank','say', 'got', 'becaus', 'look', 'make', 'time', 'ha', 'did', 'peopl', 'doe', 'sure',\n",
    "                     'need', 'whi', 'lot', 'thing', 'state', 'gener', 'great', 'anyon', 'probabl', 'hi', 'tri', 'sinc', 'ec',\n",
    "                     'way', 'said', 'll', 'write']\n",
    "\n",
    "for word in words_to_del:\n",
    "    my_stop_words.append(word)\n",
    "my_stop_words = set(my_stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['penn', 'school', 'appli', 'student', 'think', 'applic', 'colleg', 'accept', 'wharton', 'class', 'ed', 'upenn', 'onli', 'major', 'gpa', 'program', 'score', 'sat', 'admiss', 'chanc', 'work', 'help', 'essay', 'decis', 'high', 'mani', 'math', 'cours', 'ap', 'best', 'act', 'test', 'hope', 'experi', 'reject', 'univers', 'busi', 'differ', 'scienc', 'subject', 'competit', 'financi', 'commun', 'recommend', 'aid', 'grade', 'club', 'stat', 'interview', 'rank', 'senior', 'intern', 'place', 'transfer', 'rate', 'summer', 'luck', 'app', 'better', 'everyon', 'didn', 'pretti', 'come', 'question', 'read', 'ye', 'award', 'admit', 'ca', 'mean', 'research', 'ask', 'definit', 'extracurricular', 'consid', 'offer', 'incom', 'els', 'use', 'ivi', 'ii', 'doesn', 'person', 'attend', 'http', 'academ', 'submit', 'feel', 'volunt', 'post', 'job', 'kid', 'studi', 'day', 'receiv', 'talk', 'engin', 'number', 'start', 'email']\n"
     ]
    }
   ],
   "source": [
    "nlp = NLPPipe(vectorizer=TfidfVectorizer(stop_words=set(my_stop_words),max_features=15000), \n",
    "              cleaning_function=clean, \n",
    "              tokenizer=TreebankWordTokenizer().tokenize, \n",
    "              stemmer=PorterStemmer())\n",
    "\n",
    "nlp.fit(corpus)\n",
    "nlp.transform(corpus).toarray()\n",
    "top_100 = top_features(nlp, 0, 100)\n",
    "print(top_100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_to_del = ['high', 'place', 'didn', 'everyon', 'pretti', 'come', 'ye', 'ca', 'ask', 'els', 'use', 'ii', 'doesn',\n",
    "                'ivi', 'person', 'http', 'kid', 'day', 'talk', 'number', 'start']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_stop_words = add_stopwords(my_stop_words, words_to_del)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['penn', 'school', 'appli', 'student', 'think', 'applic', 'colleg', 'accept', 'wharton', 'class', 'ed', 'upenn', 'onli', 'major', 'gpa', 'program', 'score', 'sat', 'admiss', 'chanc', 'work', 'help', 'essay', 'decis', 'mani', 'math', 'cours', 'ap', 'best', 'act', 'test', 'hope', 'experi', 'reject', 'univers', 'busi', 'differ', 'scienc', 'subject', 'competit', 'financi', 'commun', 'recommend', 'aid', 'grade', 'club', 'stat', 'interview', 'rank', 'senior', 'intern', 'transfer', 'rate', 'luck', 'summer', 'app', 'better', 'question', 'read', 'award', 'admit', 'mean', 'research', 'definit', 'extracurricular', 'offer', 'consid', 'incom', 'attend', 'academ', 'submit', 'feel', 'volunt', 'post', 'job', 'studi', 'receiv', 'engin', 'email', 'degre', 'freshman', 'current', 'english', 'actual', 'week', 'howev', 'common', 'physic', 'right', 'send', 'addit', 'leadership', 'servic', 'activ', 'befor', 'level', 'teacher', 'nation', 'choic', 'new']\n"
     ]
    }
   ],
   "source": [
    "nlp = NLPPipe(vectorizer=TfidfVectorizer(stop_words=set(my_stop_words),max_features=15000), \n",
    "              cleaning_function=clean, \n",
    "              tokenizer=TreebankWordTokenizer().tokenize, \n",
    "              stemmer=PorterStemmer())\n",
    "\n",
    "nlp.fit(corpus)\n",
    "nlp.transform(corpus).toarray()\n",
    "top_100 = top_features(nlp, 0, 100)\n",
    "print(top_100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hook', 'honor', 'plan', 'love', 'junior', 'anoth', 'letter', 'histori', 'reason', 'offic', 'end', 'rd', 'someth', 'counselor', 'type', 'unweight', 'mayb', 'countri', 'hard', 'won', 'specif', 'anyth', 'rec', 'list', 'guy', 'waitlist', 'thought', 'defer', 'possibl', 'public', 'requir', 'strong', 'thread', 'presid', 'process', 'earli', 'bc', 'wonder', 'alreadi', 'world', 'point', 'campu', 'weight', 'calc', 'took', 'team', 'hour', 'harvard', 'www', 'art', 'low', 'everi', 'abl', 'went', 'wait', 'let', 'base', 'load', 'heard', 'opportun', 'tell', 'graduat', 'live', 'legaci', 'higher', 'sea', 'big', 'includ', 'pleas', 'social', 'recruit', 'dual', 'someon', 'chang', 'ethnic', 'ib', 'parent', 'inform', 'scholar', 'gender', 'option', 'life', 'bit', 'friend', 'urm', 'guess', 'strength', 'mention', 'taken', 'dure', 'close', 'select', 'undergrad', 'believ', 'sophomor', 'com', 'littl', 'especi', 'group', 'second', 'isn', 'percentil', 'matter', 'weak', 'case', 'local', 'expect', 'websit', 'internship', 'abov', 'similar', 'posit', 'famili', 'long', 'averag', 'lol', 'idea', 'fit', 'wrote', 'info', 'compar', 'import', 'agre', 'object', 'bracket', 'report', 'reflect', 'answer', 'comment', 'econom', 'non', 'semest', 'intend', 'play', 'noth', 'breakdown', 'far', 'alway', 'haven', 'edu', 'hey', 'hous', 'advic', 'learn', 'complet', 'area', 'sent', 'member', 'worri', 'columbia']\n"
     ]
    }
   ],
   "source": [
    "print(top_features(nlp, 100,250))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_to_del = ['anoth', 'anoth', 'end', 'rd', 'someth', 'mayb', 'anyth', 'possibl', 'thread', 'bc', 'wonder', 'alreadi',\n",
    "                'took', 'hour', 'www', 'everi', 'abl', 'went', 'let', 'load', 'heard', 'tell', 'live', 'sea', 'big', 'pleas',\n",
    "                'someon', 'chang', 'inform', 'bit', 'urm', 'guess', 'mention', 'taken', 'dure', 'believ', 'com', 'especi',\n",
    "                'isn', 'abov', 'lol', 'compar', 'wrote', 'agre', 'object', 'non', 'noth', 'alway', 'edu', 'hey', 'sent']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_stop_words = add_stopwords(my_stop_words, words_to_del)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hook', 'honor', 'plan', 'love', 'junior', 'letter', 'histori', 'reason', 'offic', 'counselor', 'type', 'unweight', 'countri', 'hard', 'won', 'specif', 'rec', 'waitlist', 'list', 'guy', 'thought', 'defer', 'public', 'requir', 'strong', 'presid', 'process', 'earli', 'world', 'point', 'campu', 'weight', 'calc', 'team', 'harvard', 'art', 'low', 'wait', 'base', 'opportun', 'graduat', 'legaci', 'higher', 'includ', 'social', 'dual', 'recruit', 'ethnic', 'parent', 'ib', 'scholar', 'gender', 'option', 'life', 'friend', 'strength', 'undergrad', 'select', 'close', 'sophomor', 'littl', 'group', 'second', 'percentil', 'matter', 'case', 'weak', 'expect', 'local', 'websit', 'internship', 'similar', 'famili', 'posit', 'long', 'averag', 'idea', 'fit', 'info', 'import', 'bracket', 'report', 'reflect', 'answer', 'comment', 'semest', 'econom', 'intend', 'play', 'breakdown', 'far', 'haven', 'hous', 'advic', 'learn', 'complet', 'area', 'member', 'worri', 'columbia', 'financ', 'quit', 'fall', 'fact', 'unavail', 'ab', 'male', 'manag', 'super', 'decid', 'understand', 'intel', 'usamo', 'date', 'choos', 'languag', 'relat', 'past', 'small', 'professor', 'varsiti', 'cornel', 'check', 'asian', 'kind', 'contact', 'educ', 'provid', 'overal', 'given', 'involv', 'spanish', 'happen', 'singl', 'pre', 'stanford', 'econ', 'son', 'societi', 'calculu', 'term', 'superscor', 'updat', 'bio', 'qualifi', 'lower', 'shot', 'captain', 'rang', 'organ']\n"
     ]
    }
   ],
   "source": [
    "nlp = NLPPipe(vectorizer=TfidfVectorizer(stop_words=set(my_stop_words),max_features=15000), \n",
    "              cleaning_function=clean, \n",
    "              tokenizer=TreebankWordTokenizer().tokenize, \n",
    "              stemmer=PorterStemmer())\n",
    "\n",
    "nlp.fit(corpus)\n",
    "nlp.transform(corpus).toarray()\n",
    "print(top_features(nlp, 100, 250))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_to_del = ['ab', 'kind', 'given', 'overal', 'happen', 'singl', 'pre']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_stop_words = add_stopwords(my_stop_words, words_to_del)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hook', 'honor', 'plan', 'love', 'junior', 'letter', 'histori', 'reason', 'offic', 'counselor', 'type', 'unweight', 'countri', 'hard', 'won', 'specif', 'rec', 'waitlist', 'list', 'guy', 'thought', 'defer', 'public', 'requir', 'strong', 'presid', 'process', 'earli', 'world', 'point', 'campu', 'weight', 'calc', 'team', 'harvard', 'art', 'low', 'wait', 'base', 'opportun', 'graduat', 'legaci', 'higher', 'social', 'includ', 'recruit', 'dual', 'ethnic', 'ib', 'parent', 'gender', 'scholar', 'option', 'life', 'friend', 'strength', 'select', 'undergrad', 'close', 'sophomor', 'littl', 'group', 'second', 'percentil', 'matter', 'weak', 'case', 'local', 'expect', 'websit', 'internship', 'similar', 'posit', 'long', 'famili', 'averag', 'idea', 'fit', 'info', 'import', 'bracket', 'report', 'reflect', 'answer', 'econom', 'comment', 'semest', 'intend', 'play', 'breakdown', 'far', 'haven', 'hous', 'advic', 'learn', 'complet', 'area', 'worri', 'member', 'columbia', 'financ', 'quit', 'fall', 'fact', 'unavail', 'male', 'manag', 'super', 'decid', 'understand', 'usamo', 'intel', 'date', 'languag', 'choos', 'relat', 'past', 'small', 'professor', 'varsiti', 'cornel', 'check', 'asian', 'contact', 'educ', 'provid', 'involv', 'spanish', 'econ', 'stanford', 'societi', 'son', 'term', 'calculu', 'superscor', 'updat', 'bio', 'lower', 'qualifi', 'shot', 'captain', 'rang', 'organ', 'everyth', 'undergradu', 'wouldn', 'amaz', 'bad', 'assum', 'biolog']\n"
     ]
    }
   ],
   "source": [
    "nlp = NLPPipe(vectorizer=TfidfVectorizer(stop_words=set(my_stop_words),max_features=15000), \n",
    "              cleaning_function=clean, \n",
    "              tokenizer=TreebankWordTokenizer().tokenize, \n",
    "              stemmer=PorterStemmer())\n",
    "\n",
    "nlp.fit(corpus)\n",
    "nlp.transform(corpus).toarray()\n",
    "print(top_features(nlp, 100, 250))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>academ</th>\n",
       "      <th>accept</th>\n",
       "      <th>act</th>\n",
       "      <th>activ</th>\n",
       "      <th>actual</th>\n",
       "      <th>addit</th>\n",
       "      <th>admiss</th>\n",
       "      <th>admit</th>\n",
       "      <th>advic</th>\n",
       "      <th>aid</th>\n",
       "      <th>...</th>\n",
       "      <th>weak</th>\n",
       "      <th>websit</th>\n",
       "      <th>week</th>\n",
       "      <th>weight</th>\n",
       "      <th>wharton</th>\n",
       "      <th>won</th>\n",
       "      <th>work</th>\n",
       "      <th>world</th>\n",
       "      <th>worri</th>\n",
       "      <th>yale</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.148565</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.156702</td>\n",
       "      <td>0.122244</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.372192</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.204167</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.302355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.115617</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.155389</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.11821</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16446</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16447</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.157198</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.471994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16448</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.750372</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16449</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.113844</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.160486</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16450</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.75145</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>16451 rows Ã— 250 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         academ    accept       act  activ  actual  addit    admiss     admit  \\\n",
       "0      0.148565  0.000000  0.000000    0.0     0.0    0.0  0.000000  0.000000   \n",
       "1      0.372192  0.000000  0.000000    0.0     0.0    0.0  0.000000  0.000000   \n",
       "2      0.000000  0.115617  0.000000    0.0     0.0    0.0  0.000000  0.155389   \n",
       "3      0.000000  0.000000  0.000000    0.0     0.0    0.0  0.000000  0.000000   \n",
       "4      0.000000  0.000000  0.000000    0.0     0.0    0.0  0.000000  0.000000   \n",
       "...         ...       ...       ...    ...     ...    ...       ...       ...   \n",
       "16446  0.000000  0.000000  0.000000    0.0     0.0    0.0  0.000000  0.000000   \n",
       "16447  0.000000  0.000000  0.000000    0.0     0.0    0.0  0.157198  0.000000   \n",
       "16448  0.000000  0.000000  0.750372    0.0     0.0    0.0  0.000000  0.000000   \n",
       "16449  0.000000  0.000000  0.000000    0.0     0.0    0.0  0.113844  0.000000   \n",
       "16450  0.000000  0.000000  0.000000    0.0     0.0    0.0  0.000000  0.000000   \n",
       "\n",
       "         advic  aid  ...  weak    websit  week  weight  wharton       won  \\\n",
       "0      0.00000  0.0  ...   0.0  0.000000   0.0     0.0  0.00000  0.156702   \n",
       "1      0.00000  0.0  ...   0.0  0.000000   0.0     0.0  0.00000  0.000000   \n",
       "2      0.00000  0.0  ...   0.0  0.000000   0.0     0.0  0.11821  0.000000   \n",
       "3      0.00000  0.0  ...   0.0  0.000000   0.0     0.0  0.00000  0.000000   \n",
       "4      0.00000  0.0  ...   0.0  0.000000   0.0     0.0  0.00000  0.000000   \n",
       "...        ...  ...  ...   ...       ...   ...     ...      ...       ...   \n",
       "16446  0.00000  0.0  ...   0.0  0.000000   0.0     0.0  0.00000  0.000000   \n",
       "16447  0.00000  0.0  ...   0.0  0.000000   0.0     0.0  0.00000  0.000000   \n",
       "16448  0.00000  0.0  ...   0.0  0.000000   0.0     0.0  0.00000  0.000000   \n",
       "16449  0.00000  0.0  ...   0.0  0.160486   0.0     0.0  0.00000  0.000000   \n",
       "16450  0.75145  0.0  ...   0.0  0.000000   0.0     0.0  0.00000  0.000000   \n",
       "\n",
       "           work  world  worri      yale  \n",
       "0      0.122244    0.0    0.0  0.000000  \n",
       "1      0.204167    0.0    0.0  0.302355  \n",
       "2      0.000000    0.0    0.0  0.000000  \n",
       "3      0.000000    0.0    0.0  0.000000  \n",
       "4      0.000000    0.0    0.0  0.000000  \n",
       "...         ...    ...    ...       ...  \n",
       "16446  0.000000    0.0    0.0  0.000000  \n",
       "16447  0.000000    0.0    0.0  0.471994  \n",
       "16448  0.000000    0.0    0.0  0.000000  \n",
       "16449  0.000000    0.0    0.0  0.000000  \n",
       "16450  0.000000    0.0    0.0  0.000000  \n",
       "\n",
       "[16451 rows x 250 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp = NLPPipe(vectorizer=TfidfVectorizer(stop_words=set(my_stop_words),max_features=250), \n",
    "              cleaning_function=clean, \n",
    "              tokenizer=TreebankWordTokenizer().tokenize, \n",
    "              stemmer=PorterStemmer())\n",
    "\n",
    "nlp.fit(corpus)\n",
    "X = nlp.transform(corpus).toarray()\n",
    "df = pd.DataFrame(X, columns = nlp.vectorizer.get_feature_names())\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_list = clean_list(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/meehirpathare/nltk_data...\n",
      "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_str = str(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word in my_stop_words:\n",
    "    clean_str.replace(word, \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokens = pos_tag(clean_str)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     /home/meehirpathare/nltk_data...\n",
      "[nltk_data]   Unzipping chunkers/maxent_ne_chunker.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('maxent_ne_chunker')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     /home/meehirpathare/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/words.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entities = ne_chunk(tokens)\n",
    "entities.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('stop_words', 'w') as f:\n",
    "    write = csv.writer(f)\n",
    "    write.write(row(list(my_stop_words)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
